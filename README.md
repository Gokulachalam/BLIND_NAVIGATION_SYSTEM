# BLIND_NAVIGATION_SYSTEM

## Project Description
A blind navigation system using OpenCV could utilize computer vision algorithms to interpret visual data from a camera feed and provide audio or haptic feedback to guide the user. Features such as object recognition and depth perception could enhance the system's accuracy and effectiveness.

## Libraries Used

1.Python 3
2.Open cv

## Algorithms Used

1.YOLO

## Steps Done

1.Set up a camera to capture live video feed.

2.Use OpenCV to process the video feed and extract relevant information such as object location, distance, and direction.

3.Implement an algorithm to analyze the extracted information and determine the user's location and the desired path.

4.Use audio or haptic feedback to guide the user along the desired path.

5.Continuously update the user's location and the desired path based on real-time visual data from the camera feed.

6.Integrate additional features such as object recognition and depth perception to enhance the system's accuracy and effectiveness.

7.Test and refine the system to improve its performance and user experience.

## Currently Detecting objects

person
bicycle
car
motorcycle
airplane
bus
train
truck
boat
traffic light
fire hydrant
street sign
stop sign
parking meter
bench
bird
cat
dog
horse
sheep
cow
elephant
bear
zebra
giraffe
hat
backpack
umbrella
shoe
eye glasses
handbag
tie
suitcase
frisbee
skis
snowboard
sports ball
kite
baseball bat
baseball glove
skateboard
surfboard
tennis racket
bottle
plate
wine glass
cup
fork
knife
spoon
bowl
banana
apple
sandwich
orange
broccoli
carrot
hot dog
pizza
donut
cake
chair
couch
potted plant
bed
mirror
dining table
window
desk
toilet
door
tv
laptop
mouse
remote
keyboard
cell phone
microwave
oven
toaster
sink
refrigerator
blender
book
clock
vase
scissors
teddy bear
hair drier
toothbrush
hair brush
